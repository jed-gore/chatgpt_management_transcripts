{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "772c10f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.26.5.tar.gz (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (4.62.3)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.20 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Building wheels for collected packages: openai\n",
      "  Building wheel for openai (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai: filename=openai-0.26.5-py3-none-any.whl size=67596 sha256=64b79af426ba47ddc7b55df59c46d5d3e9894c53230e134b60cc2f105c5fe783\n",
      "  Stored in directory: /Users/jedgore/Library/Caches/pip/wheels/17/e0/3d/e7f547caa758526c1a066c1fdfa4995877ef34ea0e7367010e\n",
      "Successfully built openai\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.26.5\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.10/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip3 install --upgrade openai\n",
    "#!pip3 install pandas\n",
    "#!pip3 install matplotlib\n",
    "#!pip3 install scikit-learn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "#from sklearn.metrics import confusion_matrix,plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "942dc51d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-ZmGVMk6vgcDtFx1IrNuLT3BlbkFJQm1kvuUJ6VI1DyDIDfWI\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "openai.api_key = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f2a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_snippets = []\n",
    "bad_snippets = []\n",
    "\n",
    "bad_example = 'We expect net charge-offs will average between 3.5% and 3.9% for the full year. The low end of the range is more in line with our base case, while the high end is more consistent with a weaker employment scenario.'\n",
    "good_example = 'Momentum is strong, which should help to generate double-digit revenue growth and positive operating leverage. We expect end-of-period loan growth to be in the low double digits with average loan growth somewhat higher.'\n",
    "\n",
    "for i in range(0,500):\n",
    "  completion = openai.Completion.create(engine=\"davinci\", prompt=good_example,max_tokens=120)\n",
    "  good_reviews.append(completion.choices[0]['text'])\n",
    "  print('Generating good snippet number %i'%(i))\n",
    "  completion = openai.Completion.create(engine=\"davinci\", prompt=bad_example,max_tokens=120)\n",
    "  bad_reviews.append(completion.choices[0]['text'])\n",
    "  print('Generating bad snippet number %i'%(i))\n",
    "  display = np.random.choice([0,1],p=[0.7,0.3])\n",
    "  time.sleep(3)\n",
    "  if display ==1:\n",
    "    display_good = np.random.choice([0,1],p=[0.5,0.5])\n",
    "    if display_good ==1:\n",
    "      print('Printing random good snippet')\n",
    "      print(good_reviews[-1])\n",
    "    if display_good ==0:\n",
    "      print('Printing random bad snippet')\n",
    "      print(bad_reviews[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329fda82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(np.zeros((1000,2)))\n",
    "df.columns = ['snippets','sentiment']\n",
    "df['sentiment'].loc[0:499] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2780d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['snippets'] = good_snippets+bad_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('generated_snippets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde36bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labeled_data = pd.read_csv('generated_reviews.csv').drop(columns=['Unnamed: 0'])\n",
    "labeled_data.Sentiment = labeled_data.Sentiment.astype(int)\n",
    "labeled_data = labeled_data.dropna().reset_index()\n",
    "labeled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3fd882",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = labeled_data\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#tokenized_data = tokenizer(dataset[\"Reviews\"].values.tolist(), return_tensors=\"np\", padding=True)\n",
    "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8)\n",
    "tokenized_data = vectorizer.fit_transform(dataset['snippets']).toarray()\n",
    "\n",
    "labels = np.array(dataset[\"sentiment\"])  # Label is already an array of 0 and 1\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "X = tokenized_data\n",
    "y = labels\n",
    "X_train, X_test,y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f69368",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf09c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = [{\"snippets\":\"Three things put upward pressure on our card allowance. The first factor was the continued credit normalization in our portfolio. The second factor was a modestly worse economic outlook than our assumption a quarter ago. And finally, we built allowance for the loan growth in the quarter.\",\n",
    "               \"snippets\":\"The second factor also putting upward pressure on our allowance is the impact of a modestly worse economic outlook. \",\n",
    "                \"snippets\":\"Strong consumer deposit growth throughout the quarter drove cash balances higher and allowed us to pay down prior FHLB borrowings.\"\n",
    "               }]\n",
    "target_data = pd.DataFrame(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ad638",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = target_data\n",
    "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8)\n",
    "vectorizer.fit(dataset['snippets'])\n",
    "new_data_processed = vectorizer.transform(target_data['snippet']).toarray()\n",
    "y_pred = rf.predict(new_data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb25d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = np.random.choice(range(0,len(new_data_processed)),5)\n",
    "for j in J:\n",
    "    print('Snippet number %i: \\n'%(j))\n",
    "    print(target_data['review'].loc[j])\n",
    "    print('Classified as %i (1=good, 0=bad)' %(y_pred[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=pd.DataFrame(y_pred)[0])\n",
    "plt.xticks([0,1],['bad snippets','good snippets'])\n",
    "plt.xlabel('snippets')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
